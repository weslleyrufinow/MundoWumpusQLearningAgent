{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Learning:\n",
    "    def __init__(self, size, episodes, epsilon, discount_factor):\n",
    "        self.size = size\n",
    "        self.agent = None        \n",
    "        self.wumpus = None\n",
    "        self.gold = None\n",
    "        self.pits = None\n",
    "        self.episodes = episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = [\"up\", \"right\", \"down\", \"left\"]\n",
    "        self.set_coords()\n",
    "        self.set_rewards()\n",
    "\n",
    "    def set_coords(self): #Escolhe coordenadas aleatórias para os itens do mundo\n",
    "        # Distribui poços aleatoriamente\n",
    "        num_pits = random.randint(1, self.size) #Define a qtd de poços aleatoriamente\n",
    "        self.pits = []\n",
    "        for _ in range(num_pits): #Coordenadas aleatorias pra cada poço\n",
    "            self.pits.append((random.randint(0, self.size - 1),random.randint(0, self.size - 1)))\n",
    "\n",
    "        # Distribui o Wumpus aleatoriamente\n",
    "        self.wumpus = (random.randint(0, self.size - 1),random.randint(0, self.size - 1))\n",
    "        # Distribui o Ouro aleatoriamente\n",
    "        self.gold = (random.randint(0, self.size - 1),random.randint(0, self.size - 1))         \n",
    "        \n",
    "        self.agent = (0,0) # Colocar o agente na posição inicial\n",
    "\n",
    "    def set_rewards(self):\n",
    "        # Usando numpy pra criar matrizes facilmente\n",
    "        # Matriz para armazenar as recompensas pra cada posicao\n",
    "        self.rewards = np.full((self.size, self.size), -1) #Tamanho de Size, iniciada com -1 de Reward\n",
    "        for pit in self.pits: # Atribuindo -100 de recompensa aos poços\n",
    "            self.rewards[pit] = -100\n",
    "\n",
    "        self.rewards[self.wumpus] = -150 # Atribuindo -150 de recompensa ao Wumpus\n",
    "        self.rewards[self.gold] = 200 # Atribuindo 200 de recompensa ao Ouro\n",
    "\n",
    "        # Matriz 3D para armazenar os valores Q para cada par estado-ação\n",
    "        self.q_values = np.zeros((self.size, self.size, 4)) #Iniciado com 0\n",
    "        # print(np.shape(self.rewards))\n",
    "        # print(np.shape(self.q_values))\n",
    "\n",
    "    def __str__(self):\n",
    "        output = \"  |0|1|2|3|\\n\"\n",
    "        for row in range(self.size):\n",
    "            output += \"|%d\"%(row)\n",
    "            for col in range(self.size):\n",
    "                if(row == self.agent[0] and col == self.agent[1]):\n",
    "                    output += \"|A\"\n",
    "                elif(self.rewards[row][col] == -1):\n",
    "                    output += \"| \"\n",
    "                elif(self.rewards[row][col] == -100):\n",
    "                    output += \"|P\"                    \n",
    "                elif(self.rewards[row][col] == -150):\n",
    "                    output += \"|W\"                    \n",
    "                elif(self.rewards[row][col] == 200):\n",
    "                    output += \"|G\"\n",
    "            output += \"|\\n\"\n",
    "        return output\n",
    "               \n",
    "    #Retorna True, se o estado analisado não for Poço, Wumpus ou Ouro\n",
    "    def is_terminal_state(self, row, col):\n",
    "        if self.rewards[row, col] == -1:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    #Algoritmo epsilon-greedy.\n",
    "    #Usando epsilon como indicador de probabilidade, retorna se uma ação de explotation ou exploration deve ser feita\n",
    "    def get_next_action(self, cur_row_index, cur_col_index, epsilon):\n",
    "        #Se um valor aleatório entre 0 e 1 for menor que epsilon,\n",
    "        if np.random.random() < epsilon:\n",
    "            #Escolha o \"melhor\" valor da tabela Q para esse estado\n",
    "            return np.argmax(self.q_values[cur_row_index, cur_col_index])\n",
    "        else:  #Se não, escolha uma ação aleatória\n",
    "            return np.random.randint(4)\n",
    "\n",
    "    # define uma função que obterá a próxima localização com base na ação escolhida\n",
    "    def get_next_location(self, cur_row_index, cur_col_index, action_index):\n",
    "        new_row_index = cur_row_index  # Mantém a linha atual como padrão\n",
    "        new_col_index = cur_col_index  # Mantém a coluna atual como padrão\n",
    "        \n",
    "        # Se a ação for 'up' e não estiver na primeira linha\n",
    "        if self.actions[action_index] == 'up' and cur_row_index > 0:  \n",
    "            new_row_index -= 1  # Decrementa o índice da linha para mover para cima\n",
    "        \n",
    "        # Se a ação for 'right' e não estiver na última coluna\n",
    "        elif self.actions[action_index] == 'right' and cur_col_index < self.size - 1:  \n",
    "            new_col_index += 1  # Incrementa o índice da coluna para mover para a direita\n",
    "        \n",
    "        # Se a ação for 'down' e não estiver na última linha\n",
    "        elif self.actions[action_index] == 'down' and cur_row_index < self.size - 1:  \n",
    "            new_row_index += 1  # Incrementa o índice da linha para mover para baixo\n",
    "        \n",
    "        # Se a ação for 'left' e não estiver na primeira coluna\n",
    "        elif self.actions[action_index] == 'left' and cur_col_index > 0:  \n",
    "            new_col_index -= 1  # Decrementa o índice da coluna para mover para a esquerda\n",
    "\n",
    "        return new_row_index, new_col_index  # Retorna a nova linha e coluna da próxima localização\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episodes):\n",
    "            row_index, col_index = [0, 0]  #Define a posição inicial do agente como [0, 0]\n",
    "\n",
    "            #Enquanto a posição atual não for um estado terminal\n",
    "            while not self.is_terminal_state(row_index, col_index):  \n",
    "                # Obtém a próxima ação a ser executada\n",
    "                action_index = self.get_next_action(row_index, col_index, self.epsilon)  \n",
    "                #Armazena a posição anterior do agente\n",
    "                old_row_index, old_col_index = row_index, col_index  \n",
    "                # Calcula o próxima passo do agente e põe o agente lá\n",
    "                row_index, col_index = self.get_next_location(row_index, col_index, action_index)  \n",
    "                self.agent = (row_index, col_index) #Atualiza a posição do agente\n",
    "                #Obtém a recompensa da nova localização\n",
    "                reward = self.rewards[row_index, col_index]\n",
    "                \n",
    "                # Calcula o novo valor Q, pela formula tradicional de Q-learning\n",
    "                new_q_value = reward + (self.discount_factor * np.max(self.q_values[row_index, col_index]))  \n",
    "\n",
    "                self.q_values[old_row_index, old_col_index, action_index] = new_q_value #Atualiza o valor Q da ação tomada\n",
    "          \n",
    "\n",
    "    def get_path(self, verbose=False):\n",
    "        start_row_index, start_col_index = [0, 0]  #Define a posição inicial como [0, 0]\n",
    "        \n",
    "        #Se a posição inicial for um estado terminal\n",
    "        if self.is_terminal_state(start_row_index, start_col_index):  \n",
    "            return []  # Retorna uma lista vazia, já que não há caminho a ser percorrido\n",
    "        else:\n",
    "            cur_row_index, cur_col_index = start_row_index, start_col_index #Define a posição atual como a posição inicial\n",
    "            path = []  # Lista para armazenar o caminho percorrido\n",
    "            path.append([cur_row_index, cur_col_index])  # Adiciona a posição inicial ao caminho\n",
    "\n",
    "            # Enquanto a posição atual não for um estado terminal\n",
    "            while not self.is_terminal_state(cur_row_index, cur_col_index):  \n",
    "                action_index = self.get_next_action(cur_row_index, cur_col_index, self.epsilon)  # Obtém a próxima ação a ser executada\n",
    "                cur_row_index, cur_col_index = self.get_next_location(cur_row_index, cur_col_index, action_index)  # Calcula a próxima localização\n",
    "                path.append([cur_row_index, cur_col_index])  # Adiciona a nova posição ao caminho percorrido\n",
    "                self.agent = (cur_row_index, cur_col_index)  # Atualiza a posição do agente\n",
    "\n",
    "                if verbose:  # Se o modo verbose estiver ativado\n",
    "                    print(self)  # Imprime o ambiente (representação do mundo) atual\n",
    "                    clear_output(wait=True)  # Limpa a saída do console\n",
    "                    time.sleep(0.45)  # Aguarda um curto período de tempo para visualização mais clara\n",
    "\n",
    "            if verbose:\n",
    "                print(path)  # Imprime o caminho percorrido se o modo verbose estiver ativado\n",
    "            return path  # Retorna o caminho percorrido\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon:<br>\n",
    "O parâmetro epsilon (ε) no algoritmo epsilon-greedy controla a proporção de exploração e explotação.<br>\n",
    "Ele determina a probabilidade de escolher uma ação de exploração (aleatória) em vez de uma ação de explotação (com base nas estimativas atuais de valor)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discount Factor:<br>\n",
    "O fator de desconto é um valor no intervalo [0, 1] que indica o quão \"importante\" é uma recompensa futura em comparação com uma recompensa imediata.<br>\n",
    "Um valor de discount_factor próximo de 0 indica que o agente dá mais peso às recompensas imediatas, enquanto um valor próximo de 1 indica que o agente considera recompensas futuras igualmente ou até mais importantes do que recompensas imediatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 0], [0, 1], [1, 1], [1, 2], [2, 2], [1, 2], [1, 3]]\n",
      "  |0|1|2|3|\n",
      "|0| | |W| |\n",
      "|1| | | |A|\n",
      "|2| |P| | |\n",
      "|3| | | | |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mundo_wumpus = Q_Learning(size = 4, episodes = 10000, epsilon = 0.6, discount_factor = 0.8)\n",
    "mundo_wumpus.train()\n",
    "mundo_wumpus.get_path(verbose=True)\n",
    "print(mundo_wumpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
